---
show: step
version: 1.0
enable_checker: true
---

# Sqoop 介绍、安装与操作

## 实验介绍

本节实验将对数据迁移工具 Sqoop 进行介绍。

**环境说明**

部署节点操作系统为 Ubuntu，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/opt` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/opt` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /opt`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统：Ubuntu 64 位，4 核，16G 内存
- JDK：1.8.0_292 64 位
- Hadoop：2.9.2

#### 知识点

- Sqoop 介绍
- Sqoop 安装与部署
- 使用 Sqoop 进行数据迁移

## Sqoop 介绍

#### Sqoop 简介

Sqoop 即 SQL to Hadoop，是一款方便的在传统型数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输，发展至今主要演化了两大版本：Sqoop1 和 Sqoop2。

Sqoop 工具是 Hadoop 下连接关系型数据库和 Hadoop 的桥梁，支持关系型数据库和 hive、hdfs、hbase 之间数据的相互导入，可以使用全表导入和增量导入。那么为什么选择 Sqoop 呢？

- 高效可控的利用资源，任务并行度，超时时间。
- 数据类型映射与转化，可自动进行，用户也可自定义。
- 支持多种主流数据库，MySQL、Oracle、SQL Server、DB2 等等。

#### Sqoop1 和 Sqoop2 比较

**Sqoop1 和 Sqoop2 异同**

- 两个不同的版本，完全不兼容
- 版本号划分区别
  - Apache 版本：1.4.x(Sqoop1); 1.99.x(Sqoop2)。
  - CDH 版本：Sqoop-1.4.3-cdh4(Sqoop1); Sqoop2-1.99.2-cdh4.5.0 (Sqoop2)。
- Sqoop2 比 Sqoop1 的改进
  1.  引入 Sqoop server，集中化管理 connector 等
  2.  多种访问方式：CLI，Web UI，REST API
  3.  引入基于角色的安全机制

**Sqoop1 与 Sqoop2 的架构图**

Sqoop 架构图 1

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988408517.png)

Sqoop 架构图 2

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988417189.png)

**Sqoop1 与 Sqoop2 的优缺点**

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988431084.png)

## 安装部署 Sqoop

这一章节我们将正式开始讲解安装部署 Sqoop，分步骤进行。

> **注**：该实验环境 Sqoop 已经安装完成，以下安装步骤仅仅作为演示。

### 下载 Sqoop

通过下面命令启动 hadoop 并通过 jps 查看进程：

```bash
cd /opt/hadoop-2.9.2/bin
./start-all.sh
jps
```

可以到 apache 基金 sqoop 官网 `http://sqoop.apache.org/`，选择镜像下载地址：`http://mirror.bit.edu.cn/apache/sqoop/` 下载一个稳定版本，如下图所示下载支持 Hadoop2.X 的 1.4.7 版本 gz 包：

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/d1132b73ccfe8167e00442a241546fea-0)

前置步骤

```bash
# 进入指定目录
cd /home/shiyanlou

# 下载 Sqoop
wget https://labfile.oss.aliyuncs.com/courses/237/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
```

```bash
tar -xzf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
mv sqoop-1.4.7.bin__hadoop-2.6.0 /opt/sqoop-1.4.7
```

### 设置 /etc/profile 参数

编辑 `/etc/profile` 文件，加入 sqoop 的 Home 路径和在 PATH 加入 bin 的路径：

```text
export SQOOP_HOME=/opt/sqoop-1.4.7
export PATH=$PATH:$SQOOP_HOME/bin
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988482354.png)

编译配置文件 `/etc/profile`，并确认生效。

```bash
source /etc/profile
echo $PATH
```

### 设置 bin/configure-sqoop 配置文件

修改 `bin/configure-sqoop` 配置文件。

```bash
cd /opt/sqoop-1.4.7/bin
sudo vi configure-sqoop
```

注释掉 HBase 和 Zookeeper 等检查（除非使用 HBase 和 Zookeeper 等 HADOOP 上的组件）。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988509547.png)

### 设置 conf/sqoop-env.sh 配置文件

如果不存在 `sqoop-env.sh` 文件，复制 `sqoop-env-template.sh` 文件，然后修改 `sqoop-env.sh` 配置文件。

```bash
cd /opt/sqoop-1.4.7/conf
cp sqoop-env-template.sh sqoop-env.sh
sudo vi sqoop-env.sh
```

增加如下内容：

```bash
export HADOOP_COMMON_HOME=/opt/hadoop-2.9.2
export HADOOP_MAPRED_HOME=/opt/hadoop-2.9.2
export HBASE_HOME=/opt/hbase-1.4.13
export HIVE_HOME=/opt/hive-2.3.4
export ZOOCFGDIR=/opt/zookeeper-3.4.6
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988528498.png)

设置 hadoop 运行程序所在路径和 `hadoop-*-core.jar` 路径。

```text
export HADOOP_COMMON_HOME=/opt/hadoop-2.9.2
export HADOOP_MAPRED_HOME=/opt/hadoop-2.9.2
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988537162.png)

编译配置文件 `sqoop-env.sh` 使之生效。

### 验证安装完成

输入如下命令验证是否正确安装 Sqoop，如果正确安装则出现 Sqoop 提示。

```bash
sqoop help
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988551752.png)

## 文件导入 / 导出

这一章节我们将正式开始讲解实验，分步骤进行。

### MySQL 数据导入到 HDFS 中

**启动 MySQL 服务**

查看 MySQL 服务并查看状态，如果没有启动则启动服务。

```bash
sudo service mysql status
sudo service mysql start
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988593162.png)

**查看 MySQL 中的数据表**

进入 MySQL 数据库，选择有数据的一张表查看内容，比较导出结果是否正确，输入如下命令：

```sql
mysql -uroot
mysql> show databases;
mysql> use hive;
mysql> show tables;
mysql> select * from VERSION;
```

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/3f068f0fb468a7911dde1a7856e7b5a3-0)

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/25fa33001afd588e4ed9531102fd4a83-0)


**把 MySQL 数据导入到 HDFS 中**

使用如下命令列出 MySQL 中所有数据库：

```bash
sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root
```

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/d599383dcf9387cd44d01852d9d6ac92-0)

使用如下命令把 hive 数据库 VERSION 表数据导入到 HDFS 中：

```bash
sqoop import --connect jdbc:mysql://localhost:3306/hive --username root --table VERSION -m 1
```

- `--username`：数据库用户名。
- `--password`：连接数据库密码（环境中默认为空）。
- `--table`：表名。
- `-m`：1 表示 map 数。

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/42818014ba7bb5e76623c1966140bb98-0)

**查看导出结果**

使用如下命令查看导出到 HDFS 结果，文件路径在当前用户 hadoop 目录下增加了 VERSION 表目录，查看 part-* 文件：

```bash
hadoop fs -ls /user/shiyanlou/VERSION
hadoop fs -cat /user/shiyanlou/VERSION/par*
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988678851.png)

### MySQL 数据导入到 Hive 中

**启动 metastore 和 hiveserver**

在使用 hive 之前需要启动 metastore 和 hiveserver 服务，通过如下命令启用：

```bash
hive --service metastore &
hive --service hiveserver &
```

启动用通过 jps 命令可以看到两个进行运行在后台。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988725412.png)

**从 MySQL 导入表数据到 Hive 中**

执行下面的命令之前，需要注意先把上一节实验中生成的文件夹删除掉，使用命令：

```bash
hadoop fs -rm -r /user/shiyanlou/VERSION
```

配置 Hive Conf 环境至 Sqoop：

```bash
# 拷贝所需 JAR 包
cp /opt/hive-2.3.4/lib/hive-common-2.3.4.jar /opt/sqoop-1.4.7/lib
```

然后使用如下命令把 MySQL 中 VERSION 表数据导入到 Hive 中：

```bash
sqoop import --connect jdbc:mysql://localhost:3306/hive --username root --table VERSION --hive-table MySql2Hive --hive-import -m 1
```

- `--username`：为 mysql 中的数据库连接用户名。
- `--password`：连接数据库密码（环境中默认为空）。
- `--table`：为导出表。
- `--hive-table`：导出表在 Hive 中的名称。
- `-m`：1 表示 map 数。

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/d4a009d429384a5ec70963b7011082f6-0)

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/052e46c82484110b2c13f496395f18e4-0)

从运行的日志可以看到，这个过程有两个阶段：

1. 第一个阶段是从 MySQL 中把数据保存到 HDFS 文件中。
2. 第二个阶段是从 HDFS 中把数据写入到 MySQL 中。

**查看导出结果**

登录 hive，在 hive 创建表并查看该表，命令如下：

```sql
hive
hive> show tables;
hive> desc MySql2Hive;
```

![图片描述](https://doc.shiyanlou.com/courses/237/2505524/6dcc3682ae3bbd20ab5e8b848e077ae7-0)
