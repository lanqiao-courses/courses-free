---
show: step
version: 1.0
enable_checker: true
---

# Mahout 介绍、安装与应用案例

## 实验介绍

本节实验将对 Mahout 进行介绍。

**环境说明**

> 注意：
>
> 本实验是对前述实验的延续，如果直接点“开始实验”进入则需要按实验 1 的方法配置并启动 hadoop。
>
> 实验使用的安装包、代码和数据均在 `/home/shiyanlou/install-pack` 目录下。

部署节点操作系统为 CentOS，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/app` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/app` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /app`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统： CentOS 64 位，单核，1G 内存
- JDK：64 位
- Hadoop：1.1.2

#### 知识点

- Mahout 介绍
- Mahout 环境搭建
- Mahout 问题解决

## Mahout 介绍

Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout 包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。

Mahout 的意思是大象的饲养者及驱赶者。Mahout 这个名称来源于该项目（有时）使用 Apache Hadoop —其徽标上有一头黄色的大象 —来实现可伸缩性和容错性。

Mahout 项目是由 Apache Lucene（开源搜索）社区中对机器学习感兴趣的一些成员发起的，他们希望建立一个可靠、文档翔实、可伸缩的项目，在其中实现一些常见的用于集群和分类的机器学习算法。该社区最初基于 Ng et al. 的文章 “Map-Reduce for Machine Learning on Multicore”（见 参考资料），但此后在发展中又并入了更多广泛的机器学习方法。

Mahout 的目标还包括：

- 建立一个用户和贡献者社区，使代码不必依赖于特定贡献者的参与或任何特定公司和大学的资金。
- 专注于实际用例，这与高新技术研究及未经验证的技巧相反。
- 提供高质量文章和示例。

## 搭建 Mahout 环境

这一节我们将正式开始讲解搭建环境，分步骤进行。

### 部署过程

**下载 Mahout**

在 Apache 下载最新的 Mahout 软件包，点击下载会推荐最快的镜像站点，以下为下载地址：http://archive.apache.org/dist/mahout/0.6/

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945535873.png/wm)

也可以在 `/home/shiyanlou/install-pack` 目录中找到该安装包，解压该安装包并把该安装包复制到 `/app` 目录中：

```bash
cd /home/shiyanlou/install-pack
tar -xzf mahout-distribution-0.6.tar.gz
mv mahout-distribution-0.6 /app/mahout-0.6
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945551031.png/wm)

**设置环境变量**

使用如下命令编辑 `/etc/profile` 文件：

```bash
sudo vi /etc/profile
```

声明 mahout 的 home 路径和在 path 加入 `bin` 的路径：

```bash
export MAHOUT_HOME=/app/mahout-0.6
export MAHOUT_CONF_DIR=/app/mahout-0.6/conf
export PATH=$PATH:$MAHOUT_HOME/bin
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945560908.png/wm)

编译配置文件 `/etc/profile`，并确认生效：

```bash
source /etc/profile
echo $PATH
```

**验证安装完成**

重新登录终端，确保 hadoop 集群启动，键入 `mahout --help` 命令，检查 Mahout 是否安装完好，看是否列出了一些算法：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945581044.png/wm)

### 测试例子

**下载测试数据**

下载一个文件 `synthetic_control.data`，下载地址：

```text
https://labfile.oss.aliyuncs.com/courses/237/synthetic_control.data
```

把这个文件放在 `$MAHOUT_HOME/testdata` 目录下：

```bash
cd /home/shiyanlou/install-pack/class9
wget https://labfile.oss-internal.aliyuncs.com/courses/237/synthetic_control.data
mkdir /app/mahout-0.6/testdata
mv synthetic_control.data /app/mahout-0.6/testdata
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945594571.png/wm)

**启动 Hadoop**

配置本机主机名为 hadoop，sudo 时需要输入 shiyanlou 用户的密码。将 hadoop 添加到最后一行的末尾。

```bash
sudo vim /etc/hosts
# 将hadoop添加到最后一行的末尾，修改后类似：（使用 tab 键添加空格）
# 172.17.2.98 f738b9456777 hadoop
ping hadoop
```

通过下面命令启动 hadoop 并通过 jps 查看进程：

```bash
cd /app/hadoop-1.1.2/bin
./start-all.sh
jps
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945603546.png/wm)

**使用 kmeans 算法**

使用如下命令进行 kmeans 算法测试：

```bash
cd /app/mahout-0.6/
mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945614782.png/wm)

这里需要说明下，当你看到下面的代码时以为是错的，其实不是，原因：MAHOUT_LOCAL：设置是否本地运行，如果设置该参数就不会在 hadoop 运行了，一旦设置这个参数那 HADOOP_CONF_DIR 和 HADOOP_HOME 两个参数就自动失效了。

```text
MAHOUT_LOCAL is not set, so we don't add HADOOP_CONF_DIR to classpath.
no HADOOP_HOME set , running locally
```

**查看结果**

结果会在根目录建立 output 新文件夹，如果下图结果表示 mahout 安装正确且运行正常：

```bash
cd /app/mahout-0.6/output
ll
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945629059.png/wm)

## 测试例子：运行 20newsgroup

这一章节我们将正式开始讲解实验，分步骤进行。

### 算法流程

朴素贝叶斯分类是一种十分简单的分类算法，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率哪个最大，就认为此待分类项属于哪个类别。

这二十个新闻组数据集合是收集大约 20,000 新闻组文档，均匀的分布在 20 个不同的集合。这 20 个新闻组集合采集最近流行的数据集合到文本程序中作为实验，根据机器学习技术。例如文本分类，文本聚集。我们将使用 Mahout 的 Bayes Classifier 创造一个模型，它将一个新文档分类到这 20 个新闻组集合的范例演示：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945643881.png)

### 实现过程（mahout 0.6 版本）

**下载数据并解压数据**

下载 20Newsgroups 数据集，地址为 http://qwone.com/~jason/20Newsgroups/，下载 `20news-bydate.tar.gz` 数据包，也可以在 `/home/shiyanlou/install-pack/class9` 目录中找到该测试数据文件：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945670391.png/wm)

解压 `20news-bydate.tar.gz` 数据包，解压后可以看到两个文件夹，分别为训练原始数据和测试原始数据：

```bash
cd /home/shiyanlou/install-pack/class9
tar -xzf 20news-bydate.tar.gz
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945683513.png/wm)

在 mahout 根目录下新建 data 文件夹，然后把 20news 训练原始数据和测试原始数据迁移到该文件夹下：

```bash
mkdir /app/mahout-0.6/data
mv 20news-bydate-t* /app/mahout-0.6/data
ll /app/mahout-0.6/data
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945695833.png/wm)

**建立训练集**

通过如下命令建立训练集，训练的数据在 `20news-bydate-train` 目录中，输出的训练集目录为 `bayes-train-input`：

```bash
cd /app/mahout-0.6
mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \
-p /app/mahout-0.6/data/20news-bydate-train \
-o /app/mahout-0.6/data/bayes-train-input \
-a org.apache.mahout.vectorizer.DefaultAnalyzer \
-c UTF-8
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945718229.png/wm)

**建立测试集**

通过如下命令建立训练集，训练的数据在 `20news-bydate-test` 目录中，输出的训练集目录为 `bayes-test-input`：

```bash
mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \
-p /app/mahout-0.6/data/20news-bydate-test \
-o /app/mahout-0.6/data/bayes-test-input \
-a org.apache.mahout.vectorizer.DefaultAnalyzer \
-c UTF-8
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945728940.png/wm)

**上传数据到 HDFS**

在 HDFS 中新建 `/class9/20news` 文件夹，把生成的训练集和测试集上传到 HDFS 的 `/class9/20news` 目录中：

```bash
hadoop fs -mkdir /class9/20news
hadoop fs -put /app/mahout-0.6/data/bayes-train-input /class9/20news
hadoop fs -put /app/mahout-0.6/data/bayes-test-input /class9/20news
hadoop fs -ls /class9/20news
hadoop fs -ls /class9/20news/bayes-test-input
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945741182.png/wm)

**训练贝叶斯分类器**

使用 trainclassifier 类训练在 HDFS 中 `/class9/20news/bayes-train-input` 的数据，生成的模型放到 /class9/ 20news/newsmodel 目录中：

```bash
mahout trainclassifier \
-i hdfs://hadoop:9000/class9/20news/bayes-train-input \
-o hdfs://hadoop:9000/class9/20news/newsmodel \
-type cbayes \
-ng 2
```

说明：Mahout 需要资源较多，执行命令可能会等待较长时间，请耐心静候。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945766159.png/wm)

**观察训练作业运行过程**

注：实验楼为命令行界面，无法观测到该步骤界面，以下描述仅做参考。

训练过程中在 JobTracker 页面观察运行情况，链接地址为 `http://**.***.**.***:50030/jobtracker.jsp`，训练任务四个作业，大概运行了 15 分钟左右：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945792073.png/wm)

点击查看具体作业信息：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945807326.png/wm)

map 运行情况：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945814007.png/wm)

作业运行情况：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945866063.png/wm)

**查看生成模型**

通过如下命令查看模型内容：

```bash
hadoop fs -ls /class9/20news
hadoop fs -ls /class9/20news/newsmodel
hadoop fs -ls /class9/20news/newsmodel/trainer-tfIdf
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945878234.png/wm)

**测试贝叶斯分类器**

使用 testclassifier 类训练在 HDFS 中 ./20news/bayes-test-input 的数据，使用的模型路径为 ./20news/newsmodel：

```bash
mahout testclassifier \
-m hdfs://hadoop:9000/class9/20news/newsmodel \
-d hdfs://hadoop:9000/class9/20news/bayes-test-input \
-type cbayes -ng 2 \
-source hdfs \
-method mapreduce
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945889995.png/wm)

**观察训练作业运行过程**

注：实验楼为命令行界面，无法观测到该步骤界面，以下描述仅做参考。

在执行过程中在 JobTracker 页面观察运行情况，链接地址为 `http://hadoop:50030/jobtracker.jsp`，训练任务 1 个作业，大概运行了 5 分钟左右：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945904847.png/wm)

作业的基本信息：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945919517.png/wm)

map 运行情况：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945925270.png/wm)

reduce 运行情况：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945947731.png/wm)

**查看结果**

这个混合矩阵的意思说明：上述 a 到 u 分别是代表了有 20 类别，这就是我们之前给的 20 个输入文件，列中的数据说明每个类别中被分配到的字节个数，classified 说明应该被分配到的总数。

381 0 0 0 0 9 1 0 0 0 1 0 0 2 0 1 0 0 3 0 0 | 398 a = rec.motorcycles 意思为 rec.motorcycles 本来是属于 a，有 381 篇文档被划为了 a 类，这个是正确的数据，其它的分别表示划到 b~u 类中的数目。我们可以看到其正确率为 381/398=0.9573 ，可见其正确率还是很高的了。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945966747.png/wm)

### 实现过程（mahout 0.7+ 版本）

在 0.7 版本的安装目录下 \$MAHOUT_HOME/examples/bin 下有个脚本文件 classifu-20newsgroups.sh，这个脚本中执行过程是和前面分布执行结果是一致的，只不过将各个 API 用 shell 脚本封装到一起了。从 0.7 版本开始，Mahout 移除了命令行调用的 API：prepare20newsgroups、trainclassifier 和 testclassifier，只能通过 shell 脚本执行。

执行 `$MAHOUT_HOME/examples/bin/classify-20newsgroups.sh` 四个选项中选择第一个选项，

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945982636.png/wm)

执行结果如下：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1043timestamp1433945992830.png/wm)

## 问题解决

#### 使用 mahout0.7+ 版本对 20Newsgroup 数据建立训练集时出错

使用如下命令对 20Newsgroupt 数据建立训练集时：

```bash
mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \
-p /app/mahout-0.9/data/20news-bydate-train \
-o /app/mahout-0.9/data/bayes-train-input \
-a org.apache.mahout.vectorizer.DefaultAnalyzer\
-c UTF-8
```

出现如下错误，原因在于从 0.7 版本开始，Mahout 移除了命令行调用的 prepare20newsgroups、trainclassifier 和 testclassifier API，只能通过 shell 脚本执行 `$MAHOUT_HOME/examples/bin/classify-20newsgroups.sh` 进行：

```text
14/12/7 21:31:35 WARN driver.MahoutDriver: Unable to add class: org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups
14/12/7 21:31:35 WARN driver.MahoutDriver: No org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups.props found on classpath, will use command-line arguments only
Unknown program 'org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups' chosen.
Valid program names are:
  arff.vector: : Generate Vectors from an ARFF file or directory
  baumwelch: : Baum-Welch algorithm for unsupervised HMM training
  .......
```
