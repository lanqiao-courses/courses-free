---
show: step
version: 1.0
enable_checker: true
---

# Pig 介绍、安装与应用案例

## 实验介绍

本节实验将介绍 Pig 高级查询语言的安装，并通过案例来进行教学。

**环境说明**

> 注意：
>
> 本实验是对前述实验的延续，如果直接点“开始实验”进入则需要按实验 1 的方法配置并启动 hadoop。
>
> 实验使用的安装包、代码和数据均在 `/home/shiyanlou/install-pack` 目录下。

部署节点操作系统为 CentOS，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/app` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/app` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /app`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统：CentOS 64 位，单核，1G 内存
- JDK：64 位
- Hadoop：1.1.2

#### 知识点

- Pig 查询语言
- 搭建 Pig 环境

## Pig 介绍

Pig 是 yahoo 捐献给 apache 的一个项目，使用 SQL-like 语言，是在 MapReduce 上构建的一种高级查询语言，把一些运算编译进 MapReduce 模型的 Map 和 Reduce 中。

Pig 有两种运行模式：Local 模式和 MapReduce 模式

- `Local 模式`：Pig 运行于 Local 模式，只涉及到单独的一台计算机
- `MapReduce 模式`：Pig 运行于 MapReduce 模式，需要能访问一个 Hadoop 集群，并且需要装上 HDFS

Pig 的调用方式：

- `Grunt shell 方式`：通过交互的方式，输入命令执行任务。
- `Pig script 方式`：通过 script 脚本的方式来运行任务。
- `嵌入式方式`：嵌入 java 源代码中，通过 java 调用来运行任务。

## 搭建 Pig 环境

配置本机主机名为 hadoop，sudo 时需要输入 shiyanlou 用户的密码。将 hadoop 添加到最后一行的末尾。

```bash
sudo vim /etc/hosts
# 将hadoop添加到最后一行的末尾，修改后类似：（使用 tab 键添加空格）
# 172.17.2.98 f738b9456777 hadoop
ping hadoop
```

使用如下命令启动 Hadoop：

```bash
cd /app/hadoop-1.1.2/bin
./start-all.sh
jps # 查看启动的进程，确保 NameNode 和 DataNode 都有启动
```

#### 下载并解压安装包

在 Apache 下载最新的 Pig 软件包，点击下载会推荐最快的镜像站点，以下为下载地址：

```text
http://mirrors.aliyun.com/apache/pig/
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725154113.png/wm)

也可以在 `/home/shiyanlou/install-pack` 目录中找到该安装包，解压该安装包并把该安装包复制到 `/app` 目录中。

```bash
cd /home/shiyanlou/install-pack
tar -xzf pig-0.13.0.tar.gz
mv pig-0.13.0 /app
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725167480.png/wm)

#### 设置环境变量

使用如下命令编辑 `/etc/profile` 文件：

```bash
sudo vi /etc/profile
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725184291.png/wm)

设置 pig 的 class 路径和在 path 加入 pig 的路径，其中 `PIG_CLASSPATH` 参数是设置 pig 在 MapReduce 工作模式：

```text
export PIG_HOME=/app/pig-0.13.0
export PIG_CLASSPATH=/app/hadoop-1.1.2/conf
export PATH=$PATH:$PIG_HOME/bin
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725195860.png/wm)

编译配置文件 `/etc/profile`，并确认生效。

```bash
source /etc/profile
echo $PATH
```

#### 验证安装完成

重新登录终端，确保 hadoop 集群启动，键入 `pig` 命令，应该能看到 pig 连接到 hadoop 集群的信息并且进入了 grunt shell 命令行模式：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725209548.png/wm)

## 测试例子

这一章节我们将正式开始讲解实验，分步骤进行。

### 测试例子内容

在 `/home/shiyanlou/install-pack/class7` 中有 `website_log.zip` 测试数据文件，该文件是某网站访问日志，请大家使用 pig 计算出每个 ip 的点击次数，例如：

```text
123.24.56.57 13
24.53.23.123 7
34.56.78.120 20
......
```

### 程序代码

```java
//加载 HDFS 中访问日志，使用空格进行分割，只加载 ip 列
records = LOAD 'hdfs://hadoop:9000/class7/input/website_log.txt' USING PigStorage(' ') AS (ip:chararray);

// 按照 ip 进行分组，统计每个 ip 点击数
records_b = GROUP records BY ip;
records_c = FOREACH records_b GENERATE group,COUNT(records) AS click;

// 按照点击数排序，保留点击数前 10 个的 ip 数据
records_d = ORDER records_c by click DESC;
top10 = LIMIT records_d 10;

// 把生成的数据保存到 HDFS 的 class7 目录中
STORE top10 INTO 'hdfs://hadoop:9000/class7/out';
```

### 准备数据

可以在 `/home/shiyanlou/install-pack/class7` 中找到本节使用的测试数据 `website_log.zip` 文件，使用 `unzip` 文件解压缩，然后调用 hadoop 上传本地文件命令把该文件传到 HDFS 中的 `/class7` 目录，如下图所示：

```bash
cd /home/shiyanlou/install-pack/class7
unzip website_log.zip
ll
hadoop fs -mkdir /class7/input
hadoop fs -copyFromLocal website_log.txt /class7/input
hadoop fs -cat /class7/input/website_log.txt | less
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725223134.png/wm)

### 实现过程

**输入代码**

进入 pig shell 命令行模式：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725247049.png/wm)

输入代码：（运行需要等待较长时间）

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725269288.png/wm)

**运行过程**

注：实验楼为命令行界面，无法观测到该步骤界面，以下描述仅做参考。

在执行过程中在 JobTracker 页面观察运行情况，链接地址为：`http://**.***.**.***:50030/jobtracker.jsp`。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725291992.png/wm)

点击查看具体作业信息。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725301943.png/wm)

可以观察到本次任务分为 4 个作业，每个作业都是在上一次作业的结果上进行计算。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725307993.png/wm)

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725320891.png/wm)

**运行结果**

通过以下命令查看最后的结果：

```bash
hadoop fs -ls /class7/out
hadoop fs -cat /class7/out/part-r-00000
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1035timestamp1433725331541.png/wm)
