---
show: step
version: 1.0
enable_checker: true
---

# Hadoop2.X 64 位环境搭建（选学）

## 实验介绍

本节实验将介绍搭建 Hadoop2.x 64 位环境。

**环境说明**

部署节点操作系统为 CentOS，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/app` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/app` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /app`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统：CentOS 64 位，单核，1G 内存
- JDK：64 位
- Hadoop：1.1.2

####  知识点

- Hadoop2.X 部署

## 部署 Hadooop2.X

这一章节我们将正式开始讲解部署 Hadoop2.X，分步骤进行。

### 配置 Hadoop 环境

配置本机主机名为 hadoop，sudo 时需要输入 shiyanlou 用户的密码。将 hadoop 添加到最后一行的末尾。

```bash
sudo vim /etc/hosts
# 将hadoop添加到最后一行的末尾，修改后类似：（使用 tab 键添加空格）
# 172.17.2.98 f738b9456777 hadoop
ping hadoop
```

在 Apache 网站上提供 Hadoop2.X 安装包只支持 32 位操作系统安装，如果在 64 位服务器安装会出现 3.1 的错误异常。这可以使用上一实验编译好的 `hadoop-2.2.0-bin.tar.gz` 文件作为安装包（该安装包在 `/home/shiyanlou/install-pack` 目录中可以找到）

**下载并解压 hadoop 安装包**

解压缩并移动到 `/app` 目录下：

```bash
cd /home/shiyanlou/install-pack
tar -xzf hadoop-2.2.0.tar.gz
rm -rf /app/hadoop-2.2.0
mv hadoop-2.2.0 /app
ls /app
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299112815/wm)

**在 Hadoop 目录下创建子目录**

在 `hadoop-2.2.0` 目录下创建 `tmp`、`name` 和 `data` 目录。

```bash
cd /app/hadoop-2.2.0
mkdir -p tmp hdfs hdfs/name hdfs/data
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299186597/wm)

**配置 hadoop-env.sh**

1. 打开配置文件 `hadoop-env.sh`：

```bash
cd /app/hadoop-2.2.0/etc/hadoop
sudo vi hadoop-env.sh
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299205857/wm)

2. 加入配置内容，设置了 hadoop 中 jdk 和 `hadoop/bin` 路径。

```bash
export HADOOP_CONF_DIR=/app/hadoop-2.2.0/etc/hadoop
export JAVA_HOME=/app/lib/jdk1.7.0_55
export PATH=$PATH:/app/hadoop-2.2.0/bin
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299222257/wm)

3. 配置文件 `/etc/profile`，对应 hadoop 修改为 2.2.0 版本。

```bash
sudo vi /etc/profile
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1029timestamp1514626213161.png/wm)

4. 编译配置文件 `/etc/profile` 和 `hadoop-env.sh`，并确认生效。

```bash
source /etc/profile
source hadoop-env.sh
hadoop version
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299233771/wm)

**如果显示 hadoop 版本还是 1.1.2 版本，大家可以尝试稍等一会儿刷新页面重新查看版本。**

**配置 yarn-env.sh**

打开配置文件 `yarn-env.sh`，设置 hadoop 中 jdk 路径，配置完毕后使用 `source yarn-env.sh` 编译该文件。

```bash
export JAVA_HOME=/app/lib/jdk1.7.0_55
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299255842/wm)

**配置 core-site.xml**

1. 使用如下命令打开 `core-site.xml` 配置文件。

```bash
cd /app/hadoop-2.2.0/etc/hadoop
sudo vi core-site.xml
```

2. 在配置文件中，按照如下内容进行配置。

```xml
<configuration>
   <property>
    <name>fs.default.name</name>
     <value>hdfs://hadoop:9000</value>
   </property>
   <property>
     <name>fs.defaultFS</name>
   </property>
   <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
   <property>
     <name>hadoop.tmp.dir</name>
     <value>file:/app/hadoop-2.2.0/tmp</value>
     <description>A base for other temporary directories.</description>
  </property>
   <property>
     <name>hadoop.proxyuser.hduser.hosts</name>
     <value>*</value>
   </property>
   <property>
     <name>hadoop.proxyuser.hduser.groups</name>
     <value>*</value>
   </property>
 </configuration>
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299287447/wm)

**配置 hdfs-site.xml**

1. 使用如下命令打开 `hdfs-site.xml` 配置文件。

```bash
cd /app/hadoop-2.2.0/etc/hadoop
sudo vi hdfs-site.xml
```

2. 在配置文件中，按照如下内容进行配置。

```xml
 <configuration>
   <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>hadoop:9001</value>
   </property>
   <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/app/hadoop-2.2.0/hdfs/name</value>
   </property>
   <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/app/hadoop-2.2.0/hdfs/data</value>
   </property>
   <property>
    <name>dfs.replication</name>
    <value>1</value>
   </property>
   <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
   </property>
 </configuration>
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299309438/wm)

**配置 mapred-site.xml**

1. 默认情况下不存在 `apred-site.xml` 文件，可以从模板拷贝一份，并使用如下命令打开 `mapred-site.xml` 配置文件。

```bash
cd /app/hadoop-2.2.0/etc/hadoop
cp mapred-site.xml.template mapred-site.xml
sudo vi mapred-site.xml
```

2. 在配置文件中，按照如下内容进行配置。

```xml
 <configuration>
   <property>
     <name>mapreduce.framework.name</name>
     <value>yarn</value>
   </property>
   <property>
     <name>mapreduce.jobhistory.address</name>
     <value>hadoop:10020</value>
   </property>
   <property>
     <name>mapreduce.jobhistory.webapp.address</name>
     <value>hadoop:19888</value>
   </property>
 </configuration>
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299325315/wm)

**配置 yarn-site.xml**

1. 使用如下命令打开 `yarn-site.xml` 配置文件。

```bash
cd /app/hadoop-2.2.0/etc/hadoop
sudo vi yarn-site.xml
```

2. 在配置文件中，按照如下内容进行配置。

```xml
 <configuration>
   <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
   </property>
   <property>
     <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
     <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
   <property>
     <name>yarn.resourcemanager.address</name>
     <value>hadoop:8032</value>
   </property>
   <property>
     <name>yarn.resourcemanager.scheduler.address</name>
     <value>hadoop:8030</value>
   </property>
   <property>
     <name>yarn.resourcemanager.resource-tracker.address</name>
     <value>hadoop:8031</value>
   </property>
   <property>
     <name>yarn.resourcemanager.admin.address</name>
     <value>hadoop:8033</value>
   </property>
   <property>
     <name>yarn.resourcemanager.webapp.address</name>
     <value>hadoop:8088</value>
   </property>
 </configuration>
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299450574/wm)

**配置 slaves 文件**

在 slaves 配置文件中设置从节点，这里设置为 hadoop，与 Hadoop1.X 区别的是 Hadoop2.X 不需要设置 Master。

```bash
cd /app/hadoop-2.2.0/etc/hadoop
vi slaves
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299471414/wm)

**格式化 namenode**

```bash
cd /app/hadoop-2.2.0/bin
./hdfs namenode -format
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299491126/wm)

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299517803/wm)

### 启动 Hadoop

**启动 hdfs**

```bash
cd /app/hadoop-2.2.0/sbin
./start-dfs.sh
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299545063/wm)

**验证当前进程**

使用 jps 命令查看运行进程，此时在 hadoop 上面运行的进程有：NameNode、SecondaryNameNode 和 DataNode 三个进程。

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299562771/wm)

**启动 yarn**

```bash
cd /app/hadoop-2.2.0/sbin
./start-yarn.sh
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299582263/wm)

**验证当前进行**

使用 jps 命令查看运行进程，此时在 hadoop 上运行的进程除了：NameNode、SecondaryNameNode 和 DataNode，增加了 ResourceManager 和 NodeManager 两个进程：

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299596249/wm)

### 测试 Hadoop

**创建测试目录**

```bash
cd /app/hadoop-2.2.0/bin
./hadoop fs -mkdir -p /class3/input
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299615151/wm)

**准备测试数据**

把 hadoop 本地配置目录 `/etc/hadoop` 的文件复制到 HDFS 的 `/class3/input` 目录。

```bash
cd /app/hadoop-2.2.0/bin
./hadoop fs -copyFromLocal ../etc/hadoop/* /class3/input
./hadoop fs -ls /class3/input
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299631467/wm)

**运行 wordcount 例子**

```bash
cd /app/hadoop-2.2.0/bin
./hadoop jar ../share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /class3/input /class3/output
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299676697/wm)

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299687818/wm)

**查看结果**

使用如下命令查看运行结果：

```bash
./hadoop fs -ls /class3/output/
./hadoop fs -cat /class3/output/part-r-00000 | less
```

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299702625/wm)

## 问题解决

#### CentOS 64bit 安装 Hadoop2.2.0 中出现文件编译位数异常

在安装 hadoop2.2.0 过程中出现如下异常："Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"。

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299717993/wm)

通过分析是由于 `lib/native` 目录中有些文件是在 32 位编译，无法适应 CentOS 64 位环境造成。

![图片描述信息](https://doc.shiyanlou.com/userid29778labid1029time1433299731479/wm)

有两种办法解决：

- 重新编译 hadoop，然后重新部署。
- 暂时办法是修改配置，忽略有问题的文件。
