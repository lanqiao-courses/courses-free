---
show: step
version: 1.0
enable_checker: true
---

# Spark 介绍和安装部署

## 实验介绍

本节实验将介绍 Spark 计算引擎的安装和使用，并通过案例来进行教学。

**环境说明**

> 注意：
>
> 本实验是对前述实验的延续，如果直接点“开始实验”进入则需要按实验 1 的方法配置并启动 hadoop。
>

部署节点操作系统为 Ubuntu，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/opt` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/opt` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /opt`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统：Ubuntu 64 位，4 核，16G 内存
- JDK：1.8.0_292 64 位
- Hadoop：2.9.2

### 知识点

- Spark 安装
- Spark-Shell 使用

## Spark 介绍

Spark 是一种快速、通用、可扩展的大数据分析引擎，2009 年诞生于加州大学伯克利分校 AMPLab，2010 年开源，2013 年 6 月成为 Apache 孵化项目，2014 年 2 月成为 Apache 顶级项目。目前，Spark 生态系统已经发展成为一个包含多个子项目的集合，其中包含 SparkSQL、Spark Streaming、GraphX、MLlib 等子项目，Spark 是基于内存计算的大数据并行计算框架。Spark 基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将 Spark 部署在大量廉价硬件之上，形成集群。Spark 得到了众多大数据公司的支持，这些公司包括 Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、阿里、腾讯、京东、携程、优酷土豆。当前百度的 Spark 已应用于凤巢、大搜索、直达号、百度大数据等业务；阿里利用 GraphX 构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯 Spark 集群达到 8000 台的规模，是当前已知的世界上最大的 Spark 集群。

### Spark 具有以下特点：

1. 快。

与 Hadoop 的 MapReduce 相比，Spark 基于内存的运算要快 100 倍以上，基于硬盘的运算也要快 10 倍以上。Spark 实现了高效的 DAG（Directed Acyclic Graph-有向无环图）执行引擎，可以通过基于内存来高效处理数据流。

2. 易用。

Spark 支持 Java、Python 和 Scala 的 API，还支持超过 80 种高级算法，使用户可以快速构建不同的应用。而且 Spark 支持交互式的 Python 和 Scala 的 shell，可以非常方便地在这些 shell 中来验证解决 Spark 集群中出现的问题的方法。

3. 通用。

Spark 提供了统一的解决方案。Spark 可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中集成使用。Spark 统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。

4. 兼容性。

Spark 可以非常方便地与其它的开源产品进行融合。比如，Spark 可以使用 Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器，并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署 Hadoop 集群的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，它实现了 Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用 Spark。此外，Spark 还提供了在 EC2 上部署 Standalone 的 Spark 集群的工具。

## 安装模式

Spark 的部署分为两大类，一类是本地（Local）模式，适用于测试与本地学习开发；另一类是集群模式，适用于线上项目开发。

Spark 应用程序在集群上部署运行时，可以由不同的组件为其提供资源管理调度服务（资源包括 CPU、内存等）。比如，可以使用自带的独立集群管理器（Standalone），或者使用 YARN，也可以使用 Mesos。因此，Spark 包括三种不同类型的集群部署方式，包括 Standalone、Spark on Mesos 和 Spark on YARN。

1. Standalone 模式。

与 MapReduce1.0 框架类似，Spark 框架本身也自带了完整的资源调度管理服务，可以独立部署到一个集群中，而不需要依赖其它系统来为其提供资源管理调度服务。在架构的设计上，Spark 与 MapReduce1.0 完全一致，都是由一个 Master 和若干个 Slave 构成，并且以槽（slot）作为资源分配单位。不同的是，Spark 中的槽不再像 MapReduce1.0 那样分为 Map 槽和 Reduce 槽，而是只设计了统一的一种槽提供给各种任务来使用。

2. Spark on Mesos 模式。

Mesos 是一种资源调度管理框架，可以为运行在它上面的 Spark 提供服务。Spark on Mesos 模式中，Spark 程序所需要的各种资源，都由 Mesos 负责调度。由于 Mesos 和 Spark 存在一定的血缘关系，因此，Spark 这个框架在进行设计开发的时候，就考虑到了对 Mesos 的充分支持。因此，相对而言，Spark 运行在 Mesos 上，要比运行在 YARN 上更加灵活、自然。目前，Spark 官方推荐采用这种模式，所以，许多公司在实际应用中也采用该模式。

3. Spark on YARN 模式。

Spark 可运行于 YARN 之上，与 Hadoop 进行统一部署，即“Spark on YARN”，其架构如下图所示，资源管理和调度依赖 YARN，分布式存储则依赖 HDFS。

![图片描述](https://doc.shiyanlou.com/courses/3402/1331656/029ab05f7b6e5dce21b3a2747749614a-0)

## 安装过程

这里我们已经安装了 Spark，采用的 Standalone 模式，版本为 spark-2.3.4-bin-hadoop2.7.tgz，其内置的 Scala 版本为 2.11.8。现把其安装步骤提供给大家以供参考：

1. 下载。

在 Spark 的官网上下载 spark-2.3.4-bin-hadoop2.7.tgz 安装包，上传到 `/opt` 目录下。

```bash
# 可以使用下述命令下载
wget https://labfile.oss.aliyuncs.com/courses/237/spark-2.3.4-bin-hadoop2.7.tgz
```

2. 解压改名。

```bash
shiyanlou:/opt/ $ tar -zxvf spark-2.3.4-bin-hadoop2.7.tgz
shiyanlou:/opt/ $ mv spark-2.3.4-bin-hadoop2.7 spark-2.3.4
```

3. 修改配置文件。

进入其 `config` 目录，修改 `spark-env.sh.template` 文件：

```bash
shiyanlou:/conf/ $ mv spark-env.sh.template spark-env.sh
shiyanlou:/conf/ $ vi spark-env.sh
……
export JAVA_HOME=/opt/jdk1.8
```

4. 配置环境变量。

```bash
shiyanlou:/conf/ $ vi ~/.zshrc
……
export SPARK_HOME=/opt/spark-2.3.4
export PATH=$SPARK_HOME/bin:$PATH
shiyanlou:/conf/ $ source ~/.zshrc
```

5. 启动验证。

首先启动 Hadoop，至少要把 HDFS 启动起来，YARN 可以不启动。

然后使用命令 `start-all.sh` 启动 Spark 集群：

```bash
shiyanlou:/opt/ $ spark-2.3.4/sbin/start-all.sh
```

启动进程后可以到 master 和 worker 2 个进程。还可以查看其 Web 界面：

![图片描述](https://doc.shiyanlou.com/courses/3402/1331656/74f594f28f57fc653a7d6d055f51542e-0)

6. 关闭。

```bash
shiyanlou:/opt/ $ spark-2.3.4/sbin/stop-all.sh
```

使用命令 `stop-all.sh` 来关闭 Spark 集群。

至此，Standalone 形式的 Spark 单节点已经安装部署成功。

注意：启动和关闭命令和 Hadoop 的启动与关闭命令是一样的，所以这里要通过命令路径来区分。

## Spark Shell

Spark Shell 是 Spark 自带的交互式 Shell 程序，方便用户进行交互式编程，用户可以在该命令行下用 Scala（或者 Java）编写 Spark 程序，后面我们学习 Spark 算子时，也可以在 Shell 下直接编程学习。

首先启动 Spark Shell：

```bash
shiyanlou:~/ $ spark-shell
……
scala>
```

可以看到 Spark 的版本为 2.3.4，Scala 的版本为 2.11.8，Spark context 上下文使用 sc 来代替，Spark session 用 spark 来代替。

退出使用命令“:quit”即可。

下面我们以 WordCount 为例，来看下 Spark 中如何来实现。

首先使用如下命令启动 Hadoop：

```bash
shiyanlou:~/ $ start-all.sh
```

然后往 HDFS 上传一个 `word.txt` 文件。

```bash
shiyanlou:~/ $ vi word.txt
hello world
hello hello world
kof
shiyanlou:~/ $ hadoop fs -put word.txt /
```

最后在 Spark Shell 中用 Scala 语言编写 Spark 程序如下：

```java
scala> sc.textFile("hdfs://localhost:9000/word.txt").flatMap(_.split(" ")).map((_,1)).
reduceByKey(_+_).saveAsTextFile("hdfs://localhost:9000/out")
```

其中 sc 是 Spark Context 对象，该对象是提交 Spark 程序的入口。上述代码具体解释如下：

   1. 使用 sc.textFile 方法从 HDFS 中读取文件 "hdfs://localhost:9000/word.txt"，生成一个包含文件每一行的 RDD。
   2. 使用 flatMap 方法对 RDD 进行转换，将每一行按空格拆分为单词，并生成一个新的 RDD。
   3. 使用 map 方法对每个单词生成一个键值对，其中键为单词，值初始化为 1，并生成一个新的 RDD。
   4. 使用 reduceByKey 方法根据键对键值对进行分组，并对相同键的值进行累加，生成一个新的 RDD。
   5. 使用 saveAsTextFile 方法将结果保存到 HDFS 中的输出目录 "hdfs://localhost:9000/out"，将结果以文本文件的形式保存其中。

因此，上述代码的目标是读取 HDFS 中的文本文件，对文件中的单词进行计数，并将计数结果保存到 HDFS 中的输出目录。

执行完毕后，在 HDFS 上查看输出结果。

```bash
shiyanlou:~/ $ hadoop fs -cat /out/part*
(hello,3)
(kof,1)
(world,2)
```

可以看到使用 Spark 的原生语言 Scala 来编写 WordCount 非常简洁，采用链式方式，使用了几个高阶函数（也叫算子）就可以实现了。而如果使用 Java 来编写的话，代码量则是好几倍，每一个高阶函数要么使用匿名内部类，要么使用 Lambda 表达式，而这些实际上都是函数式编程的方式，所以使用 Scala 这种混合了函数式编程与面向对象编程的语言来编写代码会非常简洁。所以本节以及后面涉及到 Spark 的代码，我们都统一使用 Scala 来编写，大家如果没有 Scala 的基础的话，可以先自行补充这一块基础知识即可。
