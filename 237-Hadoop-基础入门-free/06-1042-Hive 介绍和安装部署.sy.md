---
show: step
version: 1.0
enable_checker: true
---

# Hive 介绍和安装部署

## 实验介绍

本节实验将对 Hive 数据仓库进行介绍。

**环境说明**

> 注意：
>
> 本实验是对前述实验的延续，如果直接点“开始实验”进入则需要按实验 1 的方法配置并启动 hadoop。
>

部署节点操作系统为 Ubuntu，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/opt` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/opt` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /opt`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统：Ubuntu 64 位，4 核，16G 内存
- JDK：1.8.0_292 64 位
- Hadoop：2.9.2

#### 知识点

- Hive 介绍
- Hive 环境搭建
- Hive 常见问题解决

## Hive 介绍

Hive 是 Facebook 开发的构建于 Hadoop 集群之上的数据仓库应用，它提供了类似于 SQL 语法的 HQL 语句作为数据访问接口，这使得普通分析人员应用 Hadoop 的学习曲线变小，Hive 有如下特性：

- Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并使用 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。
- Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 的开发者来开发自定义的 Mapper 和 Reducer 处理内建的 Mapper 和 Reducer 无法完成的复杂分析工作。

### Hive 与关系数据库的区别

使用 Hive 的命令行接口很像操作关系数据库，但是 Hive 和关系数据库还是有很大的不同，Hive 与关系数据库的区别具体如下：

1. Hive 和关系数据库存储文件的系统不同，Hive 使用的是 Hadoop 的 HDFS（Hadoop 的分布式文件系统），关系数据库则是服务器本地的文件系统。
2. Hive 使用的计算模型是 MapReduce，而关系数据库则是自身的计算模型。
3. 关系数据库都是为实时查询的业务进行设计的，而 Hive 则是为海量数据做数据挖掘设计的，实时性很差；实时性的区别导致 Hive 的应用场景和关系数据库有很大的不同。
4. Hive 很容易扩展自己的存储能力和计算能力，这个是继承 Hadoop 的，而关系数据库在这个方面相比起来要差很多。

### Hive 架构

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433943328371.png)

由上图可知，Hadoop 的 MapReduce 是 Hive 架构的根基。Hive 架构包括如下组件：CLI（command line interface）、JDBC/ODBC、Thrift Server、WEB GUI、Metastore 和 Driver(Complier、Optimizer 和 Executor)，这些组件分为两大类：服务端组件和客户端组件。

**服务端组件：**

- `Driver 组件`：该组件包括 Complier、Optimizer 和 Executor，它的作用是将 HiveQL（类 SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的 MapReduce 计算框架。
- `Metastore 组件`：元数据服务组件，这个组件存储 Hive 的元数据，Hive 的元数据存储在关系数据库里，Hive 支持的关系数据库有 derby 和 mysql。元数据对于 Hive 十分重要，因此 Hive 支持把 metastore 服务独立出来，安装到远程的服务器集群里，从而解耦 Hive 服务和 metastore 服务，保证 Hive 运行的健壮性。
- `Thrift 服务`：Thrift 是 Facebook 开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口。

**客户端组件：**

- `CLI`：command line interface，命令行接口。
- `Thrift 客户端`：上面的架构图里没有写上 Thrift 客户端，但是 Hive 架构的许多客户端接口是建立在 Thrift 客户端之上，包括 JDBC 和 ODBC 接口。
- `WEBGUI`：Hive 客户端提供了一种通过网页的方式访问 Hive 所提供的服务。这个接口对应 Hive 的 hwi 组件（hive web interface），使用前要启动 hwi 服务。

## 搭建 Hive 环境


> **注**：当前实验环境 Hive 已经安装完成，以下内容仅供参考，读者无需重新配置。

### 启动 MySQL

在正式安装之前需要保证 MySQL 是正常运行，可以使用下述命令启动：

```bash
# 查看 MySQL 状态
sudo service mysql status

# 启动 MySQL 服务
sudo service mysql start
```

本节我们将正式开始讲解搭建 Hive 环境，分步骤进行。

### 安装 Hive

使用如下命令启动 Hadoop

```bash
cd /opt/hadoop-2.9.2/bin
sbin/start-all.sh
jps # 查看启动的进程，确保 NameNode 和 DataNode 都有启动
```

**解压并移动 Hive 安装包**

可以到 Apache 基金 Hive 官网 `http://hive.apache.org/downloads.html`，选择镜像下载地址：`http://mirrors.cnnic.cn/apache/hive/` 下载一个稳定版本。

也可以使用 wget 下载该安装包，解压该安装包并把该安装包复制到 `/opt` 目录中（注：实验环境中的 hive 版本没有官网更新的高）。

```bash
wget https://labfile.oss.aliyuncs.com/courses/237/apache-hive-2.3.4-bin.tar.gz
```

```bash
tar -xzf hive-2.3.4-bin.tar.gz
mv hive-2.3.4-bin /opt/hive-2.3.4
```

**配置 /etc/profile 环境变量**

使用如下命令打开 `/etc/profile` 文件：

```bash
sudo vi /etc/profile
```

设置如下参数：

```text
export HIVE_HOME=/opt/hive-2.3.4
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:$HIVE_HOME/bin
```

使配置文件生效：

```bash
source /etc/profile
echo $PATH
```

**设置 hive-env.sh 配置文件**

进入 `hive-2.3.4/conf` 目录，复制 `hive-env.sh.templaete` 为 `hive-env.sh`：

```bash
cd /opt/hive-2.3.4/conf
cp hive-env.sh.template hive-env.sh
sudo vi hive-env.sh
```

分别设置 `HADOOP_HOME` 和 `HIVE_CONF_DIR` 两个值：

```bash
export HADOOP_HOME=/opt/hadoop-2.9.2
export HIVE_CONF_DIR=/opt/hive-2.3.4/conf
```

**设置 hive-site.xml 配置文件**

复制 `hive-default.xml.templaete` 为 `hive-site.xml`：

```bash
cd /opt/hive-2.3.4/conf
cp hive-default.xml.template hive-site.xml
sudo vi hive-site.xml
```


**修改配置项**

hive 默认为 derby 数据库，需要把相关信息调整为 MySQL 数据库。

```text
<configuration>
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
</property>
</configuration>

```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944084172.png)

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944092960.png)

**（3）订正错误项**

把 `hive.metastore.schema.verification` 配置项值修改为 false。

```text
 <property>
   <name>hive.metastore.schema.verification</name>
   <value>false</value>
    <desc....>
 </property>
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944116338.png)

把 `/opt/hive-2.3.4/conf/hive-site.xml` 文件中的大概在 2000 行位置左右，把原来的 `<value>auth</auth>` 修改为 `<value>auth</value>`，如下所示：

```text
 <property>
    <name>hive.server2.thrift.sasl.qop</name>
    <value>auth</value>
    ......
  </property>
```

还需要在环境中使用命令 `sudo service mysql start` 启动 mysql。

### 验证部署

**启动 metastore 和 hiveserver**

启动 MySQL 服务：

```bash
sudo service mysql start
```

在使用 hive 之前需要启动 metastore 和 hiveserver 服务，通过如下命令启用：

```bash
hive --service metastore &
hive --service hiveserver &
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944130232.png)

通过 `jps` 命令可以看到两个进程运行在后台。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944136265.png)

**在 hive 中操作**

登录 hive，在 hive 创建表并查看该表，命令如下：

```sql
hive
hive> create table test(a string, b int);
hive> show tables;
hive> desc test;
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944147986.png)

登录 MySQL，在 TBLS 表中查看新增 test 表：

```sql
mysql -uroot
mysql> use hive;
mysql> select TBL_ID, CREATE_TIME, DB_ID, OWNER, TBL_NAME,TBL_TYPE from TBLS;
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1042timestamp1433944157594.png)
