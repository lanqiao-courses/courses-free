---
show: step
version: 1.0
enable_checker: true
---

# 爬取百度指数

## 回忆

- 上次爬了 baidu.com
- 找到了三组链接
- 然后分别遍历
- 还可以爬点什么？🤔

### 搜索

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211024-1635081650764)

- 百度指数的加密方法有时会修改
- 所以要搜索最新版本的爬虫程序

### 找到这样两篇

- https://blog.csdn.net/weixin_43933556/article/details/118163875
- https://blog.csdn.net/Buhdda/article/details/83864596

- 我们可以自己试试

### 尝试的结果

- 需要把 Cookie 中的"换成\\"

```python
import datetime
import requests


# 搜索指数数据解密
def decryption(keys, data):
    dec_dict = {}
    for j in range(len(keys) // 2):
        dec_dict[keys[j]] = keys[len(keys) // 2 + j]

    dec_data = ''
    for k in range(len(data)):
        dec_data += dec_dict[data[k]]
    return dec_data


if __name__ == "__main__":
    scenicName = 'oeasy'

    dataUrl = 'https://index.baidu.com/api/SearchApi/index?area=0&word=[[%7B%22name%22:%22' + scenicName + '%22,%22wordType%22:1%7D]]&days=365'

    keyUrl = 'https://index.baidu.com/Interface/ptbk?uniqid='
    header = {
            "Host": "index.baidu.com",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:93.0) Gecko/20100101 Firefox/93.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Referer": "http://index.baidu.com/",
            "Cookie": "BIDUPSID=5A",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "cross-site"
            }
    # 设置请求超时时间为30秒
    resData = requests.get(dataUrl, timeout=30, headers=header)
    print(resData.content)
    uniqid = resData.json()['data']['uniqid']
    print("uniqid:{}".format(uniqid))
    keyData = requests.get(keyUrl + uniqid, timeout=30, headers=header)
    print("keyData",keyData.content)
    keyData.raise_for_status()
    keyData.encoding = resData.apparent_encoding

    # 开始对json数据进行解析
    startDate = resData.json()['data']['userIndexes'][0]['all']['startDate']
    print("startDate:{}".format(startDate))
    endDate = resData.json()['data']['userIndexes'][0]['all']['endDate']
    print("endDate:{}".format(endDate))
    source = (resData.json()['data']['userIndexes'][0]['all']['data'])  # 原加密数据
    print("原加密数据:{}".format(source))
    key = keyData.json()['data']  # 密钥
    print("密钥:{}".format(key))


    res = decryption(key, source)
    print("res",type(res),res)
    resArr = res.split(",")
    print(resArr)
    dateStart = datetime.datetime.strptime(startDate, '%Y-%m-%d')
    dateEnd = datetime.datetime.strptime(endDate, '%Y-%m-%d')
    dataLs = []
    print(dateStart,dateEnd)
    while dateStart <= dateEnd:
        dataLs.append(str(dateStart))
        dateStart += datetime.timedelta(days=1)
        #print(dateStart.strftime('%Y-%m-%d'))

    ls = []
    for i in range(len(dataLs)):
        ls.append([scenicName, dataLs[i], resArr[i]])

    for i in range(len(ls)):
        print(ls[i])
```

### 原理分析

- 我们搜索数据的时候
- 真正的数据是来自于这个位置

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211107-1636262039250)

```
dataUrl = 'https://index.baidu.com/api/SearchApi/index?area=0&word=[[%7B%22name%22:%22' + scenicName + '%22,%22wordType%22:1%7D]]&days=365'
```

- 只要有 Cookie 这个数据文件我们可以从浏览器直接得到
- 我们可以直接从浏览器复制粘贴相应的 url
- 就是把他放到地址栏里就可以

### 放到地址栏里

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211107-1636262590594)

- 分成三组数据
  - 所有的
  - 智能的
  - pc 的

```
resData = requests.get(dataUrl, timeout=30, headers=header)
source = (resData.json()['data']['userIndexes'][0]['all']['data'])
```

- 这个东西是加密的
- 怎么解密呢？

### 解密

```
keyData = requests.get(keyUrl + uniqid, timeout=30, headers=header)
keyData.raise_for_status()
keyData.encoding = resData.apparent_encoding
key = keyData.json()['data']  # 密钥
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211107-1636263126958)

### 解密算法

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211107-1636263211999)

- 解密算法是写在 js 中的可以搜索到
- 需要把它变成 python 的形式

```
def decryption(keys, data):
    dec_dict = {}
    for j in range(len(keys) // 2):
        dec_dict[keys[j]] = keys[len(keys) // 2 + j]

    dec_data = ''
    for k in range(len(data)):
        dec_data += dec_dict[data[k]]
    return dec_data
mykey = "SqeKu8%kHfEm0CF528.70694%+3,-1"
mydata = "H880meS0mkS0me80mHH0muu0mkH0m%m0mHH0m8m0mmS0mSe0mH80mSS0quS0mFu0qke0mqS0mHF0mS80mqq0mFk0mFe0m8S0qke0m%u0mqm0mqk0qkk0mqq"
```

- 这个东西是和日期有关系的
- 然后需要和日期建立匹配关系

### 建立匹配关系并输出

```
dateStart = datetime.datetime.strptime(startDate, '%Y-%m-%d')
dateEnd = datetime.datetime.strptime(endDate, '%Y-%m-%d')
dataLs = []
print(dateStart,dateEnd)
while dateStart <= dateEnd:
    dataLs.append(str(dateStart))
    dateStart += datetime.timedelta(days=1)
    #print(dateStart.strftime('%Y-%m-%d'))

ls = []
for i in range(len(dataLs)):
    ls.append([scenicName, dataLs[i], resArr[i]])

for i in range(len(ls)):
    print(ls[i])
```

- 我想尝试一把返回所有的日期 2011-01-01 到今天的数据

### 控制日期

```
def decryption(keys, data):
    dec_dict = {}
    for j in range(len(keys) // 2):
        dec_dict[keys[j]] = keys[len(keys) // 2 + j]

    dec_data = ''
    for k in range(len(data)):
        dec_data += dec_dict[data[k]]
    return dec_data
mykey = "AB4+ndOF2oM9suD8%6+,02451973.-"
mydata = "oo2nos9noFsno2ono9dnosono4sno2Ono94noAsno9AnoMsnoAonoAAnOoAnoAonOOdnoAOnOOonOsMnOdonOdFnoAOnOoMnoM9nOdOnOddnOO2noMAnOoAnOd4nOodnOOOnOd9noAMno9ono94noAOnOo2noA9nOoAnOdMnOdFnOssnOOsnOssnO4dnOsonOOdnOoOnoM9no9AnOd2noMsnOdsnoAAno24nOssnOO2nOsonO2dnOFAnO44nO92nO9dnO9dnO2FnO94nO4AnOO9nOssnOFonOF4nOFAnoMMnOoonOsFnOd9nOO4nOsdnO29nOF9nOFdnOsonOsdnOFAnOosnOdAnOs9nOsAnOsFnOdsnoA2nOOFnOsOnOs4nOsOnOssnOsOnOsOnOo4nOOMnOOAnOssnOOsnOdonOOsnOsonOsAnOssnoMFnOddnOsAnOOAnOsonOF2nO2OnO2dnOsonOFMnOFsnOs4nOO4nOsAnO2snOFAnOO9nOFsnOOOnOsonOs4nOOAnO2OnO4onOF9nOFAnOFsnO2MnO49nO2OnOFFnOFOnOFonOFOnOs4nO4FnO44nOAOnOA2nO94nOAMnO9MnOM2nOAAnO4onOAdnOM2nOAFnOMFnOMOnOFFnOsonO9dnsosnso9nsd9nOMOnO99nOAOnO9dnO4MnO99nsdOnss2nsddnFoFn2oAn2AFnFO4nF9OnsF9nsFFns29ns4Ans2FnFOdnFOdnFF9n2sOn2donFOOnF22ns9Fns2Ans99ns2Mns2sns9onsM2nsMOns44ns2dns4Mns92nsAons2MnssMns2snss2ns9dns2MnsFsns2Fns4AnsOAnsOMnO2Ans2dnsAMnFosnsM9nsAAnsA9nFoonFdMnsAAns4OnsA4ns4dnsA2ns4ons49ns29nsO4ns2Fns4Ans9onFdsnsAMnFdOnsAMnFdons2OnssOnsO2ns2dnssonss9nsdsnsO2nssFnsFAnss9nsFsnsoAns2MnsFMnsOFnssMnssonso4nOM9nsOsnsoFnsosnsdMnO49nOFsnsodnsOsns2OnsF2nsFdnsO2nsOOnsd2nsOdnsdOnOAOnOAMnsd2nOMsnsdMnOMdnO4snO92nO94nO9AnsddnsoFnsosnsddnsdOnOAsnOMonO44nOAOnO9OnO9FnOM2nO94nO49nsdonOAFnOAAnO94nOAMnOA9nOA9nO9AnOAonO29nO92nOFFnO44nO4dnO24nOdFnOOdnO2MnOAOnOA4nO9AnOA2nOMdnOA4nO49nO2snO4dnO2FnO2MnO2FnO29nO24nO4OnO2snO2AnO24nOFMnOFMnO2AnO49nO2dnO94nO9OnO92nO22nO2onOsAnOsdnOsMnOsAnOsdnoMsnOFMnOOAnO2onOFdnOsMnOFAnOOonOFsnOsdnOOAnOOMnOo4nOOsnOOFnOFsnOOAnOO9nOO2no44nOdFnOOMnOFonOFAnO2MnOsAnOFFnO29nO2snO29nOFdnOFMnOs9nOFOnOFonOs2nOssnOoMnOs4nOssnOsAnO24nO4onOF9nOssnOs9nO2snOFsnOFonOsAnOs4nOsFnOdFnO42nsoOnOM2nO2OnOFsnOF2nO9FnO4onO2snOF4nO2onOF9nOs2nOFAnOFOnOFFnOo4noAMn2dFnO4MnOAsnO9MnO2AnO29nO4OnO22nO2snOF4nO2OnOoFnOFsnOsMnOs2nOO2nOo9nOF2nOsMnO4FnFAMn2F2ns9onss9ns22nsMsn2dsnFFdnFoFnFo9nFOFnFOMnFdAnsF2n2dOnFFsn2FFnAOOnAFsn49Fn4sFn4O4n2sFnFMdnF22nFs2nsAFns4Mns44nO99nFO4n2ssn4FOn2Aon4oAn2Asn2AMnFM2nFMdn9MMnFM4n2F4nFAMnF2OnFoonFdAnFd2nFosnF2AnsM9nsM2n4AOnosF4n9o9nM29n9AonooO9nodFMnMF2n942n4AMn2ddnF9Mn2s4nFAsnsA4nFFFnFFAnsA9nsMons94n2OMn22An2ddn2dsn2O4nFAsnFs4n22An4d2n2OAnFMonF2MnF9dnsosnFo4nF29nFs2nFs9nFo2n2o4nFO9ns9OnFodnsMAnsO4nsoFnss4nsodnsO4nso4nFodn9o4n2A2n499nodFFn92Mn4O9n422n44OnF92nFFsnsMsns9snsF9nsOAnss9nss9ns2Mns2AnsO2nsO4nsOs"
result = decryption(mykey,mydata)
print(len(result.split(",")))
```

- 但是这个返回的结果并不是十多年的
- 而是只有一年多的 (576 天)的数据
- 目前可以先得到一年以内的数据
- 这怎么办
- 再次搜索得到
  - https://blog.csdn.net/qq_30440287/article/details/120373441
- 虽然只可以爬一年多的数据
- 但我们可以通过程序去控制爬的是哪一年
- 到底是那一年呢？
- 我们从 2011 年开始爬

### 起止日期的控制

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211110-1636553523651)

- 上面的是原来的 dataurl
- 下面的是新的 dataurl
- 我们把他整合一下

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211110-1636553639476)

### 修改代码

```import datetime
import requests


# 搜索指数数据解密
def decryption(keys, data):
    dec_dict = {}
    for j in range(len(keys) // 2):
        dec_dict[keys[j]] = keys[len(keys) // 2 + j]

    dec_data = ''
    for k in range(len(data)):
        dec_data += dec_dict[data[k]]
    return dec_data


if __name__ == "__main__":
    scenicName = 'oeasy'

    dataUrl = 'https://index.baidu.com/api/SearchApi/index?area=0&word=[[%7B%22name%22:%22' + scenicName + '%22,%22wordType%22:1%7D]]&2011-01-01&endDate=2011-12-31'

    keyUrl = 'https://index.baidu.com/Interface/ptbk?uniqid='
    header = {
            "Host": "index.baidu.com",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:94.0) Gecko/20100101 Firefox/94.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://index.baidu.com/v2/index.html",
            "Connection": "keep-alive",
            "Cookie": "BIDUPSI...",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
            "If-Modified-Since": "Wed, 10 Nov 2021 03:34:12 GMT",
            "If-None-Match": "618b3db4-8f4",
            "Cache-Control": "max-age=0"
            }
    # 设置请求超时时间为30秒
    resData = requests.get(dataUrl, timeout=30, headers=header)
    print(resData.content)
    uniqid = resData.json()['data']['uniqid']
    print("uniqid:{}".format(uniqid))
    keyData = requests.get(keyUrl + uniqid, timeout=30, headers=header)
    print("keyData",keyData.content)
    keyData.raise_for_status()
    keyData.encoding = resData.apparent_encoding

    # 开始对json数据进行解析
    startDate = resData.json()['data']['userIndexes'][0]['all']['startDate']
    print("startDate:{}".format(startDate))
    endDate = resData.json()['data']['userIndexes'][0]['all']['endDate']
    print("endDate:{}".format(endDate))
    source = (resData.json()['data']['userIndexes'][0]['all']['data'])  # 原加密数据
    print("原加密数据:{}".format(source))
    key = keyData.json()['data']  # 密钥
    print("密钥:{}".format(key))


    res = decryption(key, source)
    print("res",type(res),res)
    resArr = res.split(",")
    print(resArr)
    dateStart = datetime.datetime.strptime(startDate, '%Y-%m-%d')
    dateEnd = datetime.datetime.strptime(endDate, '%Y-%m-%d')
    dataLs = []
    print(dateStart,dateEnd)
    while dateStart <= dateEnd:
        dataLs.append(str(dateStart))
        dateStart += datetime.timedelta(days=1)
        #print(dateStart.strftime('%Y-%m-%d'))

    ls = []
    for i in range(len(dataLs)):
        ls.append([scenicName, dataLs[i], resArr[i]])

    for i in range(len(ls)):
        print(ls[i])


```

- Cookie 需要进行处理"变成\\"
- 如果缺少包就安装
- 如果遇到 error 就去解决
- 一年一年的爬取

### 指定年份

- 现在我想定制一个指定年份的 python 文件
- 从 2011 一直爬到 2021
- 然后再合并
- 最终可以和 flourish 合并到一起
- 做成 racebar 那样的图
- 那百度指数是否可以和地区相关联？

### 地区指数

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211024-1635083970422)

- 我们可以得到一个字典
- 里面有省、地级市和县级市的信息
- 但不知道城市代码和具体城市名称的关联
- 搜索广东 913

### 找到关联

```python

import json

# 0. 全局默认变量
MAX_KEYWORD_GROUP_NUMBER = 5											 # 最多可以将MAX_KEYWORD_GROUP_NUMBER组关键词进行比较
MAX_KEYWORD_GROUP_SIZE = 3												 # 最多可以将MAX_KEYWORD_GROUP_SIZE个关键词进行组合
MAX_DATE_LENGTH = 360													 # 日期区间长度应当小于366, 取个整数就用360天即可
########################################################################

# 1. 百度指数js常用变量
## 1.1 省份区域索引表: CODE2AREA
##   - CODE2PROVINCE来源: main-vendor.xxxxxxxxxxxxxxxxxxx.js源码中直接复制可得
##   - js源码URL: e.g. http://index.baidu.com/v2/static/js/main-vendor.dbf1ed721cfef2e2755d.js
CODE2AREA = {
	901:"华东",902:"西南",903:"华中",904:"西南",905:"华北",
	906:"华中",907:"东北",908:"华中",909:"华东",910:"华东",
	911:"华北",912:"华南",913:"华南",914:"西南",915:"西南",
	916:"华东",917:"华东",918:"西北",919:"西北",920:"华北",
	921:"东北",922:"东北",923:"华北",924:"西北",925:"西北",
	926:"西北",927:"华中",928:"华东",929:"华北",930:"华南",
	931:"华南",932:"西南",933:"华南",934:"华南",
}

## 1.2 省份编号索引表: CODE2PROVINCE, PROVINCE2CODE
##   - CODE2PROVINCE来源: main-vendor.xxxxxxxxxxxxxxxxxxx.js源码中直接复制可得
##   - js源码URL: e.g. http://index.baidu.com/v2/static/js/main-vendor.dbf1ed721cfef2e2755d.js
CODE2PROVINCE = {
	901:"山东",902:"贵州",903:"江西",904:"重庆",905:"内蒙古",
	906:"湖北",907:"辽宁",908:"湖南",909:"福建",910:"上海",
	911:"北京",912:"广西",913:"广东",914:"四川",915:"云南",
	916:"江苏",917:"浙江",918:"青海",919:"宁夏",920:"河北",
	921:"黑龙江",922:"吉林",923:"天津",924:"陕西",925:"甘肃",
	926:"新疆",927:"河南",928:"安徽",929:"山西",930:"海南",
	931:"台湾",932:"西藏",933:"香港",934:"澳门",
}

PROVINCE2CODE = {province:str(code) for code,province in CODE2PROVINCE.items()}
## 1.3 城市编号索引表: CODE2CITY, CITY2CODE
##   - CODE2PROVINCE来源: main-vendor.xxxxxxxxxxxxxxxxxxx.js源码中直接复制可得
##   - js源码URL: e.g. http://index.baidu.com/v2/static/js/main-vendor.dbf1ed721cfef2e2755d.js
CODE2CITY = {
	1:"济南",2:"贵阳",3:"黔南",4:"六盘水",5:"南昌",
	6:"九江",7:"鹰潭",8:"抚州",9:"上饶",10:"赣州",
	11:"重庆",13:"包头",14:"鄂尔多斯",15:"巴彦淖尔",16:"乌海",
	17:"阿拉善盟",19:"锡林郭勒盟",20:"呼和浩特",21:"赤峰",22:"通辽",
	25:"呼伦贝尔",28:"武汉",29:"大连",30:"黄石",31:"荆州",
	32:"襄阳",33:"黄冈",34:"荆门",35:"宜昌",36:"十堰",
	37:"随州",38:"恩施",39:"鄂州",40:"咸宁",41:"孝感",
	42:"仙桃",43:"长沙",44:"岳阳",45:"衡阳",46:"株洲",
	47:"湘潭",48:"益阳",49:"郴州",50:"福州",51:"莆田",
	52:"三明",53:"龙岩",54:"厦门",55:"泉州",56:"漳州",
	57:"上海",59:"遵义",61:"黔东南",65:"湘西",66:"娄底",
	67:"怀化",68:"常德",73:"天门",74:"潜江",76:"滨州",
	77:"青岛",78:"烟台",79:"临沂",80:"潍坊",81:"淄博",
	82:"东营",83:"聊城",84:"菏泽",85:"枣庄",86:"德州",
	87:"宁德",88:"威海",89:"柳州",90:"南宁",91:"桂林",
	92:"贺州",93:"贵港",94:"深圳",95:"广州",96:"宜宾",
	97:"成都",98:"绵阳",99:"广元",100:"遂宁",101:"巴中",
	102:"内江",103:"泸州",104:"南充",106:"德阳",107:"乐山",
	108:"广安",109:"资阳",111:"自贡",112:"攀枝花",113:"达州",
	114:"雅安",115:"吉安",117:"昆明",118:"玉林",119:"河池",
	123:"玉溪",124:"楚雄",125:"南京",126:"苏州",127:"无锡",
	128:"北海",129:"钦州",130:"防城港",131:"百色",132:"梧州",
	133:"东莞",134:"丽水",135:"金华",136:"萍乡",137:"景德镇",
	138:"杭州",139:"西宁",140:"银川",141:"石家庄",143:"衡水",
	144:"张家口",145:"承德",146:"秦皇岛",147:"廊坊",148:"沧州",
	149:"温州",150:"沈阳",151:"盘锦",152:"哈尔滨",153:"大庆",
	154:"长春",155:"四平",156:"连云港",157:"淮安",158:"扬州",
	159:"泰州",160:"盐城",161:"徐州",162:"常州",163:"南通",
	164:"天津",165:"西安",166:"兰州",168:"郑州",169:"镇江",
	172:"宿迁",173:"铜陵",174:"黄山",175:"池州",176:"宣城",
	177:"巢湖",178:"淮南",179:"宿州",181:"六安",182:"滁州",
	183:"淮北",184:"阜阳",185:"马鞍山",186:"安庆",187:"蚌埠",
	188:"芜湖",189:"合肥",191:"辽源",194:"松原",195:"云浮",
	196:"佛山",197:"湛江",198:"江门",199:"惠州",200:"珠海",
	201:"韶关",202:"阳江",203:"茂名",204:"潮州",205:"揭阳",
	207:"中山",208:"清远",209:"肇庆",210:"河源",211:"梅州",
	212:"汕头",213:"汕尾",215:"鞍山",216:"朝阳",217:"锦州",
	218:"铁岭",219:"丹东",220:"本溪",221:"营口",222:"抚顺",
	223:"阜新",224:"辽阳",225:"葫芦岛",226:"张家界",227:"大同",
	228:"长治",229:"忻州",230:"晋中",231:"太原",232:"临汾",
	233:"运城",234:"晋城",235:"朔州",236:"阳泉",237:"吕梁",
	239:"海口",241:"万宁",242:"琼海",243:"三亚",244:"儋州",
	246:"新余",253:"南平",256:"宜春",259:"保定",261:"唐山",
	262:"南阳",263:"新乡",264:"开封",265:"焦作",266:"平顶山",
	268:"许昌",269:"永州",270:"吉林",271:"铜川",272:"安康",
	273:"宝鸡",274:"商洛",275:"渭南",276:"汉中",277:"咸阳",
	278:"榆林",280:"石河子",281:"庆阳",282:"定西",283:"武威",
	284:"酒泉",285:"张掖",286:"嘉峪关",287:"台州",288:"衢州",
	289:"宁波",291:"眉山",292:"邯郸",293:"邢台",295:"伊春",
	297:"大兴安岭",300:"黑河",301:"鹤岗",302:"七台河",303:"绍兴",
	304:"嘉兴",305:"湖州",306:"舟山",307:"平凉",308:"天水",
	309:"白银",310:"吐鲁番",311:"昌吉",312:"哈密",315:"阿克苏",
	317:"克拉玛依",318:"博尔塔拉",319:"齐齐哈尔",320:"佳木斯",322:"牡丹江",
	323:"鸡西",324:"绥化",331:"乌兰察布",333:"兴安盟",334:"大理",
	335:"昭通",337:"红河",339:"曲靖",342:"丽江",343:"金昌",
	344:"陇南",346:"临夏",350:"临沧",352:"济宁",353:"泰安",
	356:"莱芜",359:"双鸭山",366:"日照",370:"安阳",371:"驻马店",
	373:"信阳",374:"鹤壁",375:"周口",376:"商丘",378:"洛阳",
	379:"漯河",380:"濮阳",381:"三门峡",383:"阿勒泰",384:"喀什",
	386:"和田",391:"亳州",395:"吴忠",396:"固原",401:"延安",
	405:"邵阳",407:"通化",408:"白山",410:"白城",417:"甘孜",
	422:"铜仁",424:"安顺",426:"毕节",437:"文山",438:"保山",
	456:"东方",457:"阿坝",466:"拉萨",467:"乌鲁木齐",472:"石嘴山",
	479:"凉山",480:"中卫",499:"巴音郭楞",506:"来宾",514:"北京",
	516:"日喀则",520:"伊犁",525:"延边",563:"塔城",582:"五指山",
	588:"黔西南",608:"海西",652:"海东",653:"克孜勒苏柯尔克孜",654:"天门仙桃",
	655:"那曲",656:"林芝",657:"None",658:"防城",659:"玉树",
	660:"伊犁哈萨克",661:"五家渠",662:"思茅",663:"香港",664:"澳门",
	665:"崇左",666:"普洱",667:"济源",668:"西双版纳",669:"德宏",
	670:"文昌",671:"怒江",672:"迪庆",673:"甘南",674:"陵水黎族自治县",
	675:"澄迈县",676:"海南",677:"山南",678:"昌都",679:"乐东黎族自治县",
	680:"临高县",681:"定安县",682:"海北",683:"昌江黎族自治县",684:"屯昌县",
	685:"黄南",686:"保亭黎族苗族自治县",687:"神农架",688:"果洛",689:"白沙黎族自治县",
	690:"琼中黎族苗族自治县",691:"阿里",692:"阿拉尔",693:"图木舒克",
}
```

### 原始数据

- 找到 provinceReal 可以找到原始数据
- 然后查字典

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211024-1635084875883)

```
		"901": 423,
		"902": 109,
		"903": 233,
		"904": 197,
		"905": 75,
		"906": 304,
		"907": 156,
		"908": 284,
		"909": 230,
		"910": 213,
		"911": 283,
		"912": 182,
		"913": 1000,
		"914": 398,
		"915": 130,
		"916": 433,
		"917": 418,
		"918": 21,
		"919": 28,
		"920": 258,
		"921": 103,
		"922": 68,
		"923": 102,
		"924": 210,
		"925": 68,
		"926": 88,
		"927": 450,
		"928": 200,
		"929": 156,
		"930": 42,
		"931": 2,
		"932": 8,
		"933": 4
```

### 总结

- 这次爬了百度指数
  - 可以通过时间维度进行分割
  - 也可以通过空间维度进行分割
- 重要的是能够到网上查找源代码
  - 这很酷啊
  - 还能搜点什么呢？🤔
- 下次再说
