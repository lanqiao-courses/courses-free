---
show: step
version: 1.0
enable_checker: true
---

# xpath 路径表达式

## 回忆

- 这次真的爬了一个网站
  - oeasy.org
- 右键检查元素
  - 获取 xpath
- 爬取之后获得属性 href 的值
- 然后切片并拼接为绝对链接地址
- 能把这些链接都爬一遍么？🤔

### 遍历

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath(".")
    for element in et_target:
        print(element.tag)
        #attributes = element.attrib
        #f.write("http://localhost/oeasyorg"+attributes["href"][1:] + "\n")
```

- 运行出现了 12 次 html 节点
- 但是后面报了错误
- 最后一个 url 出了问题

### 解决

- 问题可能是由于最后一个链接后面有个`\n`
- 造成了最后的元素是一个空字符串
- 使用切片的方法解决问题

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210902-1630590728046)

### 找到 xpath

- 右键工具软件
- 复制 xpath
- 修改代码
- 按照地址
  - 输出项目

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210902-1630590988087)

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630651702331)

- 我想做的是保存
  - 课程名
  - 看教程网址

### 遍历

- 打开链接后再存储
  - 各教程名称
  - 看教程网址

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")[0:-1]
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath("/html/body/div/div/div[1]")
    et_target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    print("url----"+url)
    count = 0
    while ( count < len(et_target)):
        print(et_target[count].text)
        print(et_target2[count].text)
        print(et_target2[count].attrib["href"])
        count = count + 1
        print("count====" + str(count))
```

- 输出顺利
- 可以把各个网址输出到屏幕
- 现在需要把输出重定向到文本文件

### 输出重定向

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")[0:-1]
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath("/html/body/div/div/div[1]")
    et_target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    print("url----"+url)
    count = 0
    with open("urls.csv","at") as f:
        while ( count < len(et_target)):
            print("count====" + str(count))
            print(et_target[count].text)
            print(et_target2[count].text)
            print(et_target2[count].attrib["href"])
            s_url = et_target[count].text + "," \
                  + et_target2[count].attrib["href"] + "\n"
            f.write(s_url)
            count = count + 1
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630653569803)

### 优化代码

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")[0:-1]
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath("/html/body/div/div/div[1]")
    et_target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    count = len(et_target)
    with open("urls.csv","at") as f:
        for cur in range(0,count-1):
            print("cur====" + str(cur))
            print(et_target[cur].text)
            print(et_target2[cur].text)
            print(et_target2[cur].attrib["href"])
            s_url = et_target[cur].text + "," \
                  + et_target2[cur].attrib["href"] + "\n"
            f.write(s_url)
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630663926329)

### 回顾流程

- 我们先爬的是首页
  - 爬出很多链接
  - 然后存到 urls.txt 的里面
- 然后遍历 urls 里面的每一个地址
  - 得到课程的名字和链接
  - 然后入库
- 我现在先把整个流程合在一起
- 做成一个 py 文件

### 单文件

```python
import requests
from lxml import etree
response = requests.get("http://oeasy.org")
print(response.content)
et_html = etree.HTML(response.content)
anchors = et_html.xpath("/html/body/header/nav/ul/li/a")
l_anchors = []
for anchor in anchors:
    l_anchors.append("http://oeasy.org/"+anchor.attrib["href"][2:])

result = []
with open("result.csv","wt") as f:
    f.write("type,url\n")
    for page in l_anchors:
        response = requests.get(page)
        et_html = etree.HTML(response.content)
        target1 = et_html.xpath("/html/body/div/div/div[1]")
        target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
        count = 0
        print(len(target1))
        print(len(target2))
        while(count<len(target2)):
            print(target1[count].text,",",target2[count].attrib["href"])
            f.write(target1[count].text+","+target2[count].attrib["href"]+"\n")
            count += 1
```

### 入库

- 更新系统安装 postgres
- 启动 postgres
- 并以 postgres 登录

```
sudo apt update
sudo apt install postgresql
sudo /etc/init.d/postgres start
sudo -u postgres psql
```

- 查看所有数据库 database

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630664944685)

### 查看数据库

```
\l
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665003642)

- 建立数据库 oeasy
- 并进入

```
create database oeasy;
\l
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665050881)

### 建表并导入数据

```
\c oeasy
\dt
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665134696)

- 建表

```
create table urls(topic varchar, url varchar);
\dt
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665232801)

- 导入数据

```
\copy urls from '/home/shiyanlou/urls.csv' delimiter ',' csv ;
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665741698)

- 查询数据

```
select * from urls where topic like '%e%';
select * from urls where length(topic) > '5';
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665711812)

### 总结

- 这次真的爬了一个网站
  - oeasy.org
- 右键检查元素
  - 获取 xpath
- 爬取之后获得属性 href 的值
- 然后切片并拼接为绝对链接地址
- 并且把每一个链接都爬了一遍
- 能出去爬个百度么？🤔
- 下次再说
