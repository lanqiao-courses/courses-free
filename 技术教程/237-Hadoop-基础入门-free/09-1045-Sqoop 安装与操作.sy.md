---
show: step
version: 1.0
enable_checker: true
---

# Sqoop 介绍、安装与操作

## 实验介绍

本节实验将对数据迁移工具 Sqoop 进行介绍。

**环境说明**

> 注意：
>
> 本实验是对前述实验的延续，如果直接点“开始实验”进入则需要按实验 1 的方法配置并启动 hadoop。
>
> 实验使用的安装包、代码和数据均在 `/home/shiyanlou/install-pack` 目录下。

部署节点操作系统为 CentOS，防火墙和 SElinux 禁用，创建了一个 shiyanlou 用户并在系统根目录下创建 `/app` 目录，用于存放 Hadoop 等组件运行包。因为该目录用于安装 hadoop 等组件程序，用户对 shiyanlou 必须赋予 rwx 权限（一般做法是 root 用户在根目录下创建 `/app` 目录，并修改该目录拥有者为 shiyanlou(`chown –R shiyanlou:shiyanlou /app`)。

**Hadoop 搭建环境：**

- 虚拟机操作系统： CentOS 64 位，单核，1G 内存
- JDK：64 位
- Hadoop：1.1.2

#### 知识点

- Sqoop 介绍
- Sqoop 安装与部署
- 使用 Sqoop 进行数据迁移

## Sqoop 介绍

#### Sqoop 简介

Sqoop 即 SQL to Hadoop ，是一款方便的在传统型数据库与 Hadoop 之间进行数据迁移的工具，充分利用 MapReduce 并行特点以批处理的方式加快数据传输，发展至今主要演化了两大版本：Sqoop1 和 Sqoop2。

Sqoop 工具是 Hadoop 下连接关系型数据库和 Hadoop 的桥梁，支持关系型数据库和 hive、hdfs、hbase 之间数据的相互导入，可以使用全表导入和增量导入。那么为什么选择 Sqoop 呢？

- 高效可控的利用资源，任务并行度，超时时间。
- 数据类型映射与转化，可自动进行，用户也可自定义。
- 支持多种主流数据库，MySQL、Oracle、SQL Server、DB2 等等。

#### Sqoop1 和 Sqoop2 比较

**Sqoop1 和 Sqoop2 异同**

- 两个不同的版本，完全不兼容
- 版本号划分区别
  - Apache 版本：1.4.x(Sqoop1); 1.99.x(Sqoop2)。
  - CDH 版本：Sqoop-1.4.3-cdh4(Sqoop1); Sqoop2-1.99.2-cdh4.5.0 (Sqoop2)。
- Sqoop2 比 Sqoop1 的改进
  1.  引入 Sqoop server，集中化管理 connector 等
  2.  多种访问方式：CLI，Web UI，REST API
  3.  引入基于角色的安全机制

**Sqoop1 与 Sqoop2 的架构图**

Sqoop 架构图 1

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988408517.png)

Sqoop 架构图 2

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988417189.png)

**Sqoop1 与 Sqoop2 的优缺点**

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988431084.png)

## 安装部署 Sqoop

这一章节我们将正式开始讲解安装部署 Sqoop，分步骤进行。

### 下载 Sqoop

配置本机主机名为 hadoop，sudo 时需要输入 shiyanlou 用户的密码。将 hadoop 添加到最后一行的末尾。

```bash
sudo vim /etc/hosts
# 将hadoop添加到最后一行的末尾，修改后类似：（使用 tab 键添加空格）
# 172.17.2.98 f738b9456777 hadoop
ping hadoop
```

通过下面命令启动 hadoop 并通过 jps 查看进程：

```bash
cd /app/hadoop-1.1.2/bin
./start-all.sh
jps
```

可以到 apache 基金 sqoop 官网 `http://sqoop.apache.org/`，选择镜像下载地址：`http://mirror.bit.edu.cn/apache/sqoop/` 下载一个稳定版本，如下图所示下载支持 Hadoop1.X 的 1.4.5 版本 gz 包：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988448561.png/wm)

也可以在 `/home/shiyanlou/install-pack` 目录中找到该安装包，解压该安装包并把该安装包复制到 `/app` 目录中。

```bash
cd /home/shiyanlou/install-pack
tar -xzf sqoop-1.4.5.bin__hadoop-1.0.0.tar.gz
mv sqoop-1.4.5.bin__hadoop-1.0.0 /app/sqoop-1.4.5
ll /app
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988455155.png/wm)

### 设置 /etc/profile 参数

编辑 `/etc/profile` 文件，加入 sqoop 的 Home 路径和在 PATH 加入 bin 的路径：

```text
export SQOOP_HOME=/app/sqoop-1.4.5
export PATH=$PATH:$SQOOP_HOME/bin
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988482354.png/wm)

编译配置文件 `/etc/profile`，并确认生效。

```bash
source /etc/profile
echo $PATH
```

### 设置 bin/configure-sqoop 配置文件

修改 `bin/configure-sqoop` 配置文件。

```bash
cd /app/sqoop-1.4.5/bin
sudo vi configure-sqoop
```

注释掉 HBase 和 Zookeeper 等检查（除非使用 HBase 和 Zookeeper 等 HADOOP 上的组件）。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988509547.png/wm)

### 设置 conf/sqoop-env.sh 配置文件

如果不存在 `sqoop-env.sh` 文件，复制 `sqoop-env-template.sh` 文件，然后修改 `sqoop-env.sh` 配置文件。

```bash
cd /app/sqoop-1.4.5/conf
cp sqoop-env-template.sh sqoop-env.sh
sudo vi sqoop-env.sh
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988528498.png/wm)

设置 hadoop 运行程序所在路径和 `hadoop-*-core.jar` 路径（Hadoop1.X 需要配置）。

```text
export HADOOP_COMMON_HOME=/app/hadoop-1.1.2
export HADOOP_MAPRED_HOME=/app/hadoop-1.1.2
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988537162.png/wm)

编译配置文件 `sqoop-env.sh` 使之生效。

### 验证安装完成

输入如下命令验证是否正确安装 sqoop，如果正确安装则出现 sqoop 提示。

```bash
sqoop help
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988551752.png/wm)

## 文件导入 / 导出

这一章节我们将正式开始讲解实验，分步骤进行。

### MySql 数据导入到 HDFS 中

如果没有安装 MySql，请参照第 8 课 3.1 进行安装。

**下载 MySql 驱动**

到 MySql 官网进入下载页面：http://dev.mysql.com/downloads/connector/j/ ，选择所需要的版本进行下载，这里下载的 zip 格式的文件，然后在本地解压：

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988568882.png/wm)

也可以在 `/home/shiyanlou/install-pack` 目录中找到该安装包，把 MySql 驱动包使用如下命令放到 Sqoop 的 lib 目录下。

```bash
cd /home/shiyanlou/install-pack
cp mysql-connector-java-5.1.22-bin.jar /app/sqoop-1.4.5/lib
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988581378.png/wm)

**启动 MySql 服务**

查看 MySql 服务并查看状态，如果没有启动则启动服务。

```bash
sudo service mysql status
sudo service mysql start
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988593162.png/wm)

**查看 MySql 中的数据表**

进入 MySql 数据库，选择有数据的一张表查看内容，比较导出结果是否正确，输入如下命令：

```sql
mysql -uhive -phive
mysql> show databases;
mysql> use hive;
mysql> show tables;
mysql> select TBL_ID, CREATE_TIME, DB_ID, OWNER, TBL_NAME,TBL_TYPE from TBLS;
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988631637.png/wm)

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988644070.png/wm)

**把 MySql 数据导入到 HDFS 中**

使用如下命令列出 MySql 中所有数据库：

```bash
sqoop list-databases --connect jdbc:mysql://hadoop:3306/ --username hive --password hive
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988655926.png/wm)

使用如下命令把 hive 数据库 TBLS 表数据导入到 HDFS 中：

```bash
sqoop import --connect jdbc:mysql://hadoop:3306/hive --username hive --password hive --table TBLS -m 1
```

- `--username`：数据库用户名。
- `--password`：连接数据库密码。
- `--table`：表名。
- `-m`：1 表示 map 数。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988667183.png/wm)

**查看导出结果**

使用如下命令查看导出到 HDFS 结果，文件路径在当前用户 hadoop 目录下增加了 TBLS 表目录，查看 part-m-00000 文件：

```bash
hadoop fs -ls /user/shiyanlou/TBLS
hadoop fs -cat /user/shiyanlou/TBLS/part-m-00000
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988678851.png/wm)

### MySql 数据导入到 Hive 中

**启动 metastore 和 hiveserver**

在使用 hive 之前需要启动 metastore 和 hiveserver 服务，通过如下命令启用：

```bash
hive --service metastore &
hive --service hiveserver &
```

启动用通过 jps 命令可以看到两个进行运行在后台。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988725412.png/wm)

**从 MySql 导入表数据到 Hive 中**

执行下面的命令之前，需要注意先把上一节实验中生成的文件夹删除掉，使用命令：

```bash
hadoop fs -rmr /user/shiyanlou/TBLS
```

然后使用如下命令把 MySql 中 TBLS 表数据导入到 Hive 中：

```bash
sqoop import --connect jdbc:mysql://hadoop:3306/hive --username hive --password hive --table TBLS --hive-table MySql2Hive --hive-import -m 1
```

- `--username`：为 mysql 中的数据库连接用户名。
- `--password`：为 mysql 中的数据库连接密码。
- `--table`：为导出表。
- `--hive-table`：导出表在 Hive 中的名称。
- `-m`：1 表示 map 数。

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988744843.png/wm)

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988753664.png/wm)

从运行的日志可以看到，这个过程有两个阶段：

1. 第一个阶段是从 MySql 中把数据保存到 HDFS 文件中。
2. 第二个阶段是从 HDFS 中把数据写入到 MySql 中。

**查看导出结果**

登录 hive，在 hive 创建表并查看该表，命令如下：

```sql
hive
hive> show tables;
hive> desc MySql2Hive;
```

![此处输入图片的描述](https://doc.shiyanlou.com/document-uid29778labid1045timestamp1433988775534.png/wm)
